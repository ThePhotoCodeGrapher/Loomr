# Copy to .env and fill with your values for local runs.
# This file is safe to commit; do NOT commit your real .env.

#############################################
# Runtime environment variables for Loomr   #
# Loaded by messaging_service/main.py       #
# DO NOT put registry tokens here; use      #
# GitHub Actions Secrets for CI/CD.         #
#############################################

# --- Core bot config ---
# Telegram bot token (from @BotFather)
TELEGRAM_BOT_TOKEN=
# Optional: your bot username or @handle for deep links (used by InvitesPlugin)
TELEGRAM_BOT_ID=
# Admin chat/user id to receive notifications from event handlers
ADMIN_CHAT_ID=

# --- API/Webhook auth ---
# Bearer token used by /deliver and demo endpoints in API server and ProductCatalog delivery
DELIVER_BEARER=
# Secret checked by minimal webhook server (x-webhook-secret or ?secret=)
WEBHOOK_SECRET=

# --- Telegram payments (Wallet Pay) ---
# Provider token from @Wallet (Wallet Pay) for Telegram invoice demos
TELEGRAM_PROVIDER_TOKEN_WALLET=

# --- Admin tools ---
# Optional secret to allow admin self-registration in chat via /admin_register
ADMIN_REGISTER_SECRET=

# --- Support plugin (optional) ---
# Group id to post new support tickets (if omitted, admins get DMs)
SUPPORT_GROUP_ID=

# --- ProductCatalog delivery HTTP auth ---
# Used in messaging_service/config/config.yaml under products.delivery.http.headers
# Authorization: Bearer ${DELIVER_BEARER}

# --- Crypto watchers (optional) ---
# USDT watchers per chain
TUSDT_WATCH_ADDRESS=
ETHERSCAN_API_KEY=
BSCSCAN_API_KEY=
POLY_USD_ADDRESS=
POLYGONSCAN_API_KEY=

# TON watcher
TON_WALLET=
TONAPI_KEY=

# --- File router HTTP forward (optional) ---
FILE_FORWARD_URL=
FILE_FORWARD_TOKEN=

#############################################
# CI/CD NOTE                                #
# PyPI token, Docker Hub tokens, and npm    #
# tokens belong in GitHub → Settings →      #
# Secrets and variables → Actions, e.g.:    #
#   PYPI_API_TOKEN, DOCKERHUB_USERNAME,     #
#   DOCKERHUB_TOKEN, NPM_TOKEN              #
#############################################

# --- Ollama local LLM (optional) ---
# Host where Ollama listens
OLLAMA_HOST=http://127.0.0.1:11434
# Model to use (suggested: llama3:8b, qwen2:7b, etc.)
OLLAMA_MODEL=llama3:8b
# System prompt file to focus the assistant
QUESTIONARY_PATH=messaging_service/config/questionary.md
# Enable simple RAG (not yet implemented here; reserved)
# OLLAMA_RAG_ENABLED=false
# OLLAMA_RAG_EMBED_MODEL=nomic-embed-text
